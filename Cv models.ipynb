{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bigger-sheet",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "from sklearn import preprocessing, metrics\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import std\n",
    "import os, pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn import metrics, ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "incident-disorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_class(row, option):\n",
    "        \n",
    "    if option == 'PETandDX':\n",
    "        if row['AV45+'] == 1 and row['DX'] == 'Dementia':\n",
    "            return 1\n",
    "        if row['AV45+'] == 0 and row['DX'] == 'CN':\n",
    "            return 0\n",
    "        if np.isnan(row['AV45+']):\n",
    "            if row['PIB+'] == 1 and row['DX'] == 'Dementia':\n",
    "                return 1\n",
    "            if row['PIB+'] == 0 and row['DX'] == 'CN':\n",
    "                return 0\n",
    "    \n",
    "    if option == 'PET':\n",
    "        if row['AV45+'] == 1:\n",
    "            return 1\n",
    "        if row['AV45+'] == 0:\n",
    "            return 0\n",
    "        if np.isnan(row['AV45+']):\n",
    "            if row['PIB+'] == 1:\n",
    "                return 1\n",
    "            if row['PIB+'] == 0:\n",
    "                return 0\n",
    "    \n",
    "    if option == 'LOAD':\n",
    "        if row['Phenotype'] == 2:\n",
    "            return 1\n",
    "        if row['Phenotype'] == 1:\n",
    "            return 0\n",
    "        \n",
    "def create_class_ADNI(df, option):\n",
    "\n",
    "    \n",
    "    df['y'] = df.apply (lambda row: label_class(row, option), axis=1)\n",
    "\n",
    "    to_drop = list(df.columns)[-27:-1]\n",
    "    df.drop(columns=to_drop, inplace=True)\n",
    "    df_notna = df[df['y'].notna()]\n",
    "\n",
    "    # print('Class distribution:')\n",
    "    # print(df['y'].value_counts())\n",
    "\n",
    "    return df_notna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "minor-bearing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>APOE</th>\n",
       "      <th>BAX</th>\n",
       "      <th>CHRNA7</th>\n",
       "      <th>ENO1</th>\n",
       "      <th>GSK3B</th>\n",
       "      <th>IGF1</th>\n",
       "      <th>INSR</th>\n",
       "      <th>PICALM</th>\n",
       "      <th>PSEN1</th>\n",
       "      <th>SOD2</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>003_S_1057</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>003_S_0908</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>003_S_1122</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136_S_0873</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130_S_0886</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>023_S_4501</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>070_S_4692</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>005_S_4707</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>003_S_4350</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141_S_4232</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>726 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            APOE  BAX  CHRNA7  ENO1  GSK3B  IGF1  INSR  PICALM  PSEN1  SOD2  \\\n",
       "003_S_1057   2.0  0.0     0.0   0.0    0.0   1.0   0.0     0.0    0.0   1.0   \n",
       "003_S_0908   2.0  0.0     0.0   0.0    0.0   0.0   0.0     0.0    0.0   1.0   \n",
       "003_S_1122   1.0  0.0     0.0   0.0    0.0   0.0   0.0     0.0    0.0   0.0   \n",
       "136_S_0873   2.0  0.0     0.0   0.0    0.0   0.0   0.0     0.0    0.0   1.0   \n",
       "130_S_0886   2.0  0.0     0.0   0.0    0.0   0.0   0.0     0.0    0.0   0.0   \n",
       "...          ...  ...     ...   ...    ...   ...   ...     ...    ...   ...   \n",
       "023_S_4501   2.0  0.0     0.0   0.0    0.0   0.0   0.0     0.0    0.0   1.0   \n",
       "070_S_4692   2.0  0.0     0.0   0.0    0.0   0.0   0.0     0.0    0.0   1.0   \n",
       "005_S_4707   2.0  0.0     0.0   0.0    0.0   0.0   1.0     0.0    0.0   1.0   \n",
       "003_S_4350   1.0  0.0     0.0   0.0    0.0   0.0   0.0     0.0    0.0   1.0   \n",
       "141_S_4232   1.0  0.0     0.0   0.0    0.0   0.0   0.0     0.0    0.0   1.0   \n",
       "\n",
       "              y  \n",
       "003_S_1057  1.0  \n",
       "003_S_0908  0.0  \n",
       "003_S_1122  0.0  \n",
       "136_S_0873  1.0  \n",
       "130_S_0886  1.0  \n",
       "...         ...  \n",
       "023_S_4501  1.0  \n",
       "070_S_4692  1.0  \n",
       "005_S_4707  1.0  \n",
       "003_S_4350  0.0  \n",
       "141_S_4232  0.0  \n",
       "\n",
       "[726 rows x 11 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data/table_datasets/AD_PPI_missense_ADNI_labeled.csv', index_col = 0)\n",
    "data_wclass = create_class_ADNI(data, 'PET')\n",
    "data_wclass.loc[:, (data_wclass != 0).any(axis=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "legitimate-associate",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data_wclass.drop(columns=['y'])\n",
    "x = x['APOE']\n",
    "\n",
    "y = data_wclass['y']\n",
    "x.index = x.index.str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "crude-kitchen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 3 candidates, totalling 30 fits\n",
      "RandomForestClassifier(n_estimators=50)\n",
      "Acc 0.684931506849315\n",
      "AUC 0.65625\n",
      "\n",
      "Fitting 10 folds for each of 3 candidates, totalling 30 fits\n",
      "RandomForestClassifier(n_estimators=50)\n",
      "Acc 0.6301369863013698\n",
      "AUC 0.6215701219512195\n",
      "\n",
      "Fitting 10 folds for each of 3 candidates, totalling 30 fits\n",
      "RandomForestClassifier(n_estimators=50)\n",
      "Acc 0.5616438356164384\n",
      "AUC 0.5914634146341464\n",
      "\n",
      "Fitting 10 folds for each of 3 candidates, totalling 30 fits\n",
      "RandomForestClassifier(n_estimators=500)\n",
      "Acc 0.6986301369863014\n",
      "AUC 0.7339939024390243\n",
      "\n",
      "Fitting 10 folds for each of 3 candidates, totalling 30 fits\n",
      "RandomForestClassifier(n_estimators=5000)\n",
      "Acc 0.7808219178082192\n",
      "AUC 0.7660060975609756\n",
      "\n",
      "Fitting 10 folds for each of 3 candidates, totalling 30 fits\n",
      "RandomForestClassifier(n_estimators=50)\n",
      "Acc 0.6164383561643836\n",
      "AUC 0.6467225609756098\n",
      "\n",
      "Fitting 10 folds for each of 3 candidates, totalling 30 fits\n",
      "RandomForestClassifier(n_estimators=50)\n",
      "Acc 0.5833333333333334\n",
      "AUC 0.5621557828481512\n",
      "\n",
      "Fitting 10 folds for each of 3 candidates, totalling 30 fits\n",
      "RandomForestClassifier(n_estimators=50)\n",
      "Acc 0.6666666666666666\n",
      "AUC 0.7029897718332021\n",
      "\n",
      "Fitting 10 folds for each of 3 candidates, totalling 30 fits\n",
      "RandomForestClassifier(n_estimators=500)\n",
      "Acc 0.5694444444444444\n",
      "AUC 0.5908733280881197\n",
      "\n",
      "Fitting 10 folds for each of 3 candidates, totalling 30 fits\n",
      "RandomForestClassifier(n_estimators=50)\n",
      "Acc 0.6527777777777778\n",
      "AUC 0.614870180959874\n",
      "\n",
      "0.6486895161290323 0.0632727433104859\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for i in range(10):\n",
    "\n",
    "    i += 1\n",
    "\n",
    "    f = open(f'data/splits/10Fold_CV_PET/k{i}_PET.pkl', 'rb')\n",
    "    split = pickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "    tr_idx = split['train']\n",
    "    te_idx = split['valid']\n",
    "\n",
    "    x_train, x_test = x.iloc[tr_idx], x.iloc[te_idx]\n",
    "    y_train, y_test = y[tr_idx], y[te_idx]\n",
    "\n",
    "    x_train = np.array(x_train).reshape(-1, 1)\n",
    "    x_test = np.array(x_test).reshape(-1, 1)\n",
    "\n",
    "    param_grid = [{'n_estimators': [50, 500, 5000]}]\n",
    "    rnd_clf = RandomForestClassifier()\n",
    "    \n",
    "    grid_search = GridSearchCV(rnd_clf, param_grid, cv=10, scoring='roc_auc', n_jobs = -1, verbose = 3)\n",
    "    grid_search.fit(x_train, y_train)\n",
    "    \n",
    "    print(grid_search.best_estimator_)\n",
    "    \n",
    "    y_prob_rf = grid_search.predict_proba(x_test)\n",
    "    y_pred_rf = grid_search.predict(x_test)\n",
    "    print('Acc', metrics.accuracy_score(y_test, y_pred_rf))\n",
    "    print('AUC', metrics.roc_auc_score(y_test, y_prob_rf[:, 1]))\n",
    "    \n",
    "    results.append(metrics.roc_auc_score(y_test, y_prob_rf[:, 1]))\n",
    "    print()\n",
    "\n",
    "results = np.array(results, np.float)\n",
    "print(np.mean(results), np.std(results))\n",
    "    \n",
    "#     tn, fp, fn, tp = metrics.confusion_matrix(y_test, y_pred_rf).ravel()\n",
    "#     ax = plt.subplot()\n",
    "#     heatmap([[tn, fp], [fn, tp]], annot=True, fmt='g', ax=ax, center=1)\n",
    "#     ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels');\n",
    "#     ax.set_title('Confusion Matrix');\n",
    "#     ax.xaxis.set_ticklabels(['0', '1']); ax.yaxis.set_ticklabels(['0', '1']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "greater-convert",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 5 candidates, totalling 50 fits\n",
      "{'alpha': 0.0001, 'penalty': 'l1'}\n",
      "Acc 0.4383561643835616\n",
      "AUC 0.5\n",
      "\n",
      "Fitting 10 folds for each of 5 candidates, totalling 50 fits\n",
      "{'alpha': 0.01, 'penalty': 'l1'}\n",
      "Acc 0.4383561643835616\n",
      "AUC 0.5\n",
      "\n",
      "Fitting 10 folds for each of 5 candidates, totalling 50 fits\n",
      "{'alpha': 0.0001, 'penalty': 'l1'}\n",
      "Acc 0.4383561643835616\n",
      "AUC 0.5\n",
      "\n",
      "Fitting 10 folds for each of 5 candidates, totalling 50 fits\n",
      "{'alpha': 0.0001, 'penalty': 'l1'}\n",
      "Acc 0.4383561643835616\n",
      "AUC 0.5\n",
      "\n",
      "Fitting 10 folds for each of 5 candidates, totalling 50 fits\n",
      "{'alpha': 0.01, 'penalty': 'l1'}\n",
      "Acc 0.4383561643835616\n",
      "AUC 0.5\n",
      "\n",
      "Fitting 10 folds for each of 5 candidates, totalling 50 fits\n",
      "{'alpha': 0.001, 'penalty': 'l1'}\n",
      "Acc 0.6164383561643836\n",
      "AUC 0.6310975609756098\n",
      "\n",
      "Fitting 10 folds for each of 5 candidates, totalling 50 fits\n",
      "{'alpha': 0.01, 'penalty': 'l1'}\n",
      "Acc 0.4444444444444444\n",
      "AUC 0.5082612116443745\n",
      "\n",
      "Fitting 10 folds for each of 5 candidates, totalling 50 fits\n",
      "{'alpha': 0.01, 'penalty': 'l1'}\n",
      "Acc 0.6666666666666666\n",
      "AUC 0.6797797010228166\n",
      "\n",
      "Fitting 10 folds for each of 5 candidates, totalling 50 fits\n",
      "{'alpha': 0.0001, 'penalty': 'l1'}\n",
      "Acc 0.5694444444444444\n",
      "AUC 0.5826121164437452\n",
      "\n",
      "Fitting 10 folds for each of 5 candidates, totalling 50 fits\n",
      "{'alpha': 0.0001, 'penalty': 'l1'}\n",
      "Acc 0.6527777777777778\n",
      "AUC 0.6361132966168371\n",
      "\n",
      "0.5537863886703384 0.0678213788186964\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for i in range(10):\n",
    "\n",
    "    i += 1\n",
    "\n",
    "    f = open(f'data/splits/10Fold_CV_PET/k{i}_PET.pkl', 'rb')\n",
    "    split = pickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "    tr_idx = split['train']\n",
    "    te_idx = split['valid']\n",
    "\n",
    "    x_train, x_test = x.iloc[tr_idx], x.iloc[te_idx]\n",
    "    y_train, y_test = y[tr_idx], y[te_idx]\n",
    "\n",
    "    x_train = np.array(x_train).reshape(-1, 1)\n",
    "    x_test = np.array(x_test).reshape(-1, 1)\n",
    "    \n",
    "    scaler = preprocessing.StandardScaler().fit(x_train)\n",
    "    x_train_scaled = scaler.transform(x_train)\n",
    "    x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "    param_grid = [{'penalty' : ['l1'],'alpha': [0.0001, 0.001, 0.01, 0.1, 1]}]\n",
    "    pcp_clf = Perceptron()\n",
    "    \n",
    "    grid_search_pcp = GridSearchCV(pcp_clf, param_grid, cv=10, scoring='roc_auc', n_jobs = -1, verbose = 3)\n",
    "    grid_search_pcp.fit(x_train, y_train)\n",
    "    print(grid_search_pcp.best_params_)\n",
    "\n",
    "    y_pred_perceptron = grid_search_pcp.predict(x_test_scaled)\n",
    "    print('Acc', metrics.accuracy_score(y_test, y_pred_perceptron))\n",
    "    print('AUC', metrics.roc_auc_score(y_test, y_pred_perceptron))\n",
    "    \n",
    "    results.append(metrics.roc_auc_score(y_test, y_pred_perceptron))\n",
    "    print()\n",
    "\n",
    "\n",
    "results = np.array(results, np.float)\n",
    "print(np.mean(results), np.std(results))\n",
    "    \n",
    "#     tn, fp, fn, tp = metrics.confusion_matrix(y_test, y_pred_rf).ravel()\n",
    "#     ax = plt.subplot()\n",
    "#     heatmap([[tn, fp], [fn, tp]], annot=True, fmt='g', ax=ax, center=1)\n",
    "#     ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels');\n",
    "#     ax.set_title('Confusion Matrix');\n",
    "#     ax.xaxis.set_ticklabels(['0', '1']); ax.yaxis.set_ticklabels(['0', '1']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "adult-snapshot",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc 0.6575342465753424\n",
      "AUC 0.7278963414634146\n",
      "\n",
      "Acc 0.5753424657534246\n",
      "AUC 0.6375762195121951\n",
      "\n",
      "Acc 0.5616438356164384\n",
      "AUC 0.5457317073170732\n",
      "\n",
      "Acc 0.547945205479452\n",
      "AUC 0.6882621951219512\n",
      "\n",
      "Acc 0.6164383561643836\n",
      "AUC 0.7629573170731707\n",
      "\n",
      "Acc 0.6164383561643836\n",
      "AUC 0.6154725609756098\n",
      "\n",
      "Acc 0.5972222222222222\n",
      "AUC 0.5881195908733281\n",
      "\n",
      "Acc 0.6666666666666666\n",
      "AUC 0.6565696302124311\n",
      "\n",
      "Acc 0.5694444444444444\n",
      "AUC 0.5743509047993706\n",
      "\n",
      "Acc 0.5833333333333334\n",
      "AUC 0.6085759244689221\n",
      "\n",
      "0.5746102970102281 0.05129279677327344\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for i in range(10):\n",
    "\n",
    "    i += 1\n",
    "\n",
    "    f = open(f'data/splits/10Fold_CV_PET/k{i}_PET.pkl', 'rb')\n",
    "    split = pickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "    tr_idx = split['train']\n",
    "    te_idx = split['valid']\n",
    "\n",
    "    x_train, x_test = x.iloc[tr_idx], x.iloc[te_idx]\n",
    "    y_train, y_test = y[tr_idx], y[te_idx]\n",
    "\n",
    "    x_train = np.array(x_train).reshape(-1, 1)\n",
    "    x_test = np.array(x_test).reshape(-1, 1)\n",
    "    \n",
    "    scaler = preprocessing.StandardScaler().fit(x_train)\n",
    "    x_train_scaled = scaler.transform(x_train)\n",
    "    x_test_scaled = scaler.transform(x_test)\n",
    "    \n",
    "    logreg = LogisticRegression()\n",
    "    logreg.fit(x_train_scaled, y_train)\n",
    "\n",
    "    y_prob_logreg = logreg.predict_proba(x_test_scaled)\n",
    "    y_pred_logreg = logreg.predict(x_test_scaled)\n",
    "    print('Acc', metrics.accuracy_score(y_test, y_pred_logreg))\n",
    "    print('AUC', metrics.roc_auc_score(y_test, y_prob_logreg[:, 1]))\n",
    "    \n",
    "    results.append(metrics.roc_auc_score(y_test, y_pred_logreg))\n",
    "    print()\n",
    "\n",
    "results = np.array(results, np.float)\n",
    "print(np.mean(results), np.std(results))\n",
    "    \n",
    "#     tn, fp, fn, tp = metrics.confusion_matrix(y_test, y_pred_rf).ravel()\n",
    "#     ax = plt.subplot()\n",
    "#     heatmap([[tn, fp], [fn, tp]], annot=True, fmt='g', ax=ax, center=1)\n",
    "#     ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels');\n",
    "#     ax.set_title('Confusion Matrix');\n",
    "#     ax.xaxis.set_ticklabels(['0', '1']); ax.yaxis.set_ticklabels(['0', '1']);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
