{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, datetime, pickle\n",
    "import networks.randomize_network as rn\n",
    "from create_datasets import create_nx_datasets\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Create random networks**\n",
    "\n",
    "Create 100 random networks for each randomization method (\"Shuffled\" and \"Rewired\") and for each gene set (AD and NDD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diseases = ['AD', 'ND']\n",
    "\n",
    "for disease in diseases:\n",
    "    infile = f'data/{disease}_STRING_PPI_edgelist_biggest.txt'\n",
    "    original = rn.load_graph(infile)\n",
    "\n",
    "    for i in range(1, 101):\n",
    "        shuffled = rn.shuffle_nodes(original)\n",
    "        nx.write_edgelist(shuffled, f'data/random_networks/shuffled/{disease}_PPI_rand{i}_edgelist.txt')\n",
    "\n",
    "        rewired = rn.generate_RDPN(infile)\n",
    "        nx.write_edgelist(shuffled, f'data/random_networks/rewired/{disease}_PPI_rand{i}_edgelist.txt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Create graph datasets**\n",
    "\n",
    "Create the correspoding graph-datasets for each random network. This takes times because the code builds 800 different datasets (2 randomisation methods x 2 gene sets x 100 random networks x 2 different targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'ADNI'\n",
    "targets = ['PET', 'PETandDX']\n",
    "diseases = ['AD', 'ND']\n",
    "networks = ['shuffled', 'rewired']\n",
    "\n",
    "for target in targets:\n",
    "    for disease in diseases:\n",
    "        for network in networks:\n",
    "            for i in range(1, 101):\n",
    "\n",
    "                outdir = f'data/graph_datasets/{target}/{network}'\n",
    "\n",
    "                start_time = datetime.datetime.now()\n",
    "                print()\n",
    "\n",
    "                result_nodes = create_nx_datasets.main('data', dataset, target, disease, network, 'missense', i)\n",
    "                print('Coding: number of missense variants per node')\n",
    "\n",
    "                outfile = f'{outdir}/{disease}_PPI_rand{i}_missense.pkl'\n",
    "                print('Resulting dataset saved at:', outfile)\n",
    "                print()\n",
    "\n",
    "                with open(outfile, 'wb') as f:\n",
    "                    pickle.dump(result_nodes, f)\n",
    "\n",
    "                result_nodes_time = datetime.datetime.now()\n",
    "                print('Processing time:', result_nodes_time - start_time)\n",
    "                print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Graph classification with GNNs**\n",
    "\n",
    "We then evaluated and tested different GNNs in the framework called [GraphGym](https://github.com/snap-stanford/GraphGym) (You *et al.*, 2020).\n",
    "\n",
    "Configuration and grid files employed are in the subdirectory [graphgym_files](graphgym_files). The models' configuration in this case was the same than the original GNNs.\n",
    "\n",
    "Summarized results obtained by GraphGym and other models are in [results/GNN_comparison](results/GNN_comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Statistical analysis original performance *vs.* random performances**\n",
    "We computed p-values with a 1-sample t-test comparing each original run against of all the random runs (100 random datasets x 3 runs = 300 runs) of each randomization method for each target and gene set.\n",
    "\n",
    "For this, we previously extracted the performance values of each run. Because the huge size (GB) of all files produced by GraphGym, we summarized in one file all runs for each randomization method and classification task proposed. \n",
    "\n",
    "- Results for PET target with datasets using [Shuffled and Rewired methods](results/GNN_random_results/PET/)\n",
    "- Results for PET&DX target with datasets using [Shuffled and Rewired methods](results/GNN_random_results/PETandDX/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = {\n",
    "    'AD_PET': [0.6898, 0.7180, 0.7294],\n",
    "    'ND_PET': [0.7050, 0.6349, 0.7143],\n",
    "    'AD_PETandDX': [0.6825, 0.7302, 0.7143],\n",
    "    'ND_PETandDX': [0.7937, 0.8532, 0.6389]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pvalues(original_runs, infile):\n",
    "\n",
    "    data = data = pd.read_csv(infile, index_col='random')\n",
    "    data.drop(columns=['epoch'], inplace=True)\n",
    "\n",
    "    all_list = data.values.tolist()\n",
    "    all_runs = [item for sublist in all_list for item in sublist]\n",
    "    print(len(all_runs))\n",
    "    \n",
    "    pval1 = stats.ttest_1samp(all_runs, original_runs[0], alternative='less')[1]\n",
    "    pval2 = stats.ttest_1samp(all_runs, original_runs[1], alternative='less')[1]\n",
    "    pval3 = stats.ttest_1samp(all_runs, original_runs[2], alternative='less')[1]\n",
    "\n",
    "    return pval1, pval2, pval3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = ['PET', 'PETandDX']\n",
    "randoms = ['Shuffled', 'Rewired']\n",
    "diseases = ['AD', 'ND']\n",
    "\n",
    "for target in targets:\n",
    "    for random in randoms:\n",
    "        print(f'{target} target - {random} graph datasets')\n",
    "        for disease in diseases:\n",
    "            original_values = results_dict[f'{disease}_{target}']\n",
    "            pval1, pval2, pval3 = compute_pvalues(original_values, f'results/GNN_models_results/{target}/{target}_{disease}_{random}_results.csv')\n",
    "\n",
    "            print(f'{disease} network')\n",
    "            print('Original run #1 vs. 300 random runs:', pval1)\n",
    "            print('Original run #2 vs. 300 random runs:', pval2)\n",
    "            print('Original run #3 vs. 300 random runs:', pval3)\n",
    "            print()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5e0466fb38f8b8207bee76f1d5f01e5f82e3be8917a80d900a987709dfd81e7a"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 ('graphgym')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
